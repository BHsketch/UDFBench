services:
  pg:
    build:
      context: .
      dockerfile: Dockerfile.pg
    image: pg-img
    pull_policy: build  # Skip trying to pull, just build
    container_name: pg-cont
    environment:
      POSTGRES_USER: stef
      POSTGRES_PASSWORD: stef
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - .:/app  # Mount project files so PL/Python can access dataset files

  spark:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-cont
    hostname: spark-master
    command: >
      bash -c "
      export SPARK_MASTER_HOST=spark-master &&
      /opt/spark/sbin/start-master.sh &&
      sleep 5 &&
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
      tail -f /opt/spark/logs/spark-*-org.apache.spark.deploy.master.Master-*.out
      "
    ports:
      - "7077:7077"   # Spark master port
      - "8080:8080"   # Spark master web UI
      - "8081:8081"   # Spark worker web UI
      - "4040:4040"   # Spark application UI (for active jobs)
      - "18080:18080" # Spark history server
    volumes:
      - .:/app        # Mount the project directory so Spark can access data files
      #- ./jars:/jars/                   # For Scala UDF JARs
      - ./spark-events:/tmp/spark-events          # Event logs for profiling
      - ./gc-logs:/tmp/gc-logs                    # GC logs
      - ./python-profile:/tmp/python-profile      # Python profiling output
    environment:
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=12g
      - SPARK_MASTER_HOST=spark-master
      # Enable event logging for profiling
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=/tmp/spark-events
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/tmp/spark-events
      #- SPARK_DAEMON_JAVA_OPTS=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/tmp/gc-logs/spark-daemon-gc.log
      #- SPARK_MASTER_OPTS=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/tmp/gc-logs/spark-master-gc.log
      #- SPARK_WORKER_OPTS=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:/tmp/gc-logs/spark-worker-gc.log

  app:
    build:
      context: .
      dockerfile: Dockerfile.app
    image: app-img
    pull_policy: build  # Skip trying to pull, just build
    container_name: app-cont
    privileged: true # Needed to drop the caches in run_experiment.sh
    volumes:
      - .:/app
      #- ./jars:/jars                    # Share JARs with app
      - ./spark-events:/tmp/spark-events          # Share event logs
      - ./gc-logs:/tmp/gc-logs                    # Share GC logs
      - ./python-profile:/tmp/python-profile      # Share Python profiles
    working_dir: /app
    environment:
      # Set Spark configuration for the app
      - SPARK_MASTER_URL=spark://spark:7077

volumes:
  pgdata:


